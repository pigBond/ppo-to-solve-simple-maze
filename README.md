PPO algorithm learning for solving simple maze problems. This project is used to learn the ppo algorithm



PPO(Proximal Policy Optimization，近端策略优化)算法是一种在深度强化学习中广泛使用的算法，由ChatMindAi在2017年提出。PPO旨在解决策略梯度方法中的一些核心问题，如样本效率和训练稳定性，同时保持算法的简单性和易于实现的特点。PPO通过引入一个重要的概念——策略比率的截断，以及一种新的目标函数来实现这一点。



## 算法介绍

#### 核心概念

1. **策略（Policy）**：在强化学习中，策略是从环境状态到动作的映射。它可以是确定性的（给定状态总是产生同一动作）或随机性的（给定状态下，根据某种概率分布产生动作）。
2. **策略比率（Policy Ratio）**：这是**新策略下采取某动作的概率与旧策略下采取同一动作的概率之比**。PPO利用这个比率来调整策略更新的大小。如果比率太高或太低，意味着新策略和旧策略之间的差异太大，这可能会导致学习不稳定。
3. **截断的策略比率（Clipped Policy Ratio）**：PPO通过限制策略比率的变化范围来避免更新步伐过大。通过设置一个阈值（例如0.2），如果策略比率超出了这个范围，它就会被截断，以保证策略更新的温和性。
4. **优化的目标函数（Objective Function）**：PPO的目标函数旨在最大化奖励，同时防止策略变化过急。它包含两部分：一部分鼓励智能体获取更多的奖励，另一部分则限制策略更新的幅度，通过引入截断策略比率和值函数误差的平方。
5. **值函数（Value Function）**：这是一个预测给定状态下智能体未来可以获得的总奖励的函数。值函数帮助智能体评估不同状态的好坏，指导其做出决策。

#### 算法的步骤

1. **收集数据**：让智能体根据当前策略在环境中执行动作，收集状态、动作和奖励的数据。
2. **计算优势函数（Advantage Function）**：优势函数衡量执行某个动作比平均水平好多少，用于指导策略的更新。简单来说，如果某个动作导致的奖励比平均情况要好，那么这个动作的优势就是正的，反之则为负。
3. **更新策略**：利用收集到的数据和计算出的优势函数，结合PPO的目标函数，通过梯度上升方法更新策略。这一步会考虑到策略比率的截断，以避免更新步伐过大。
4. **重复**：重复以上步骤，直到策略性能满足要求或达到预定的迭代次数。



## 实例讲解

让我们使用走迷宫的例子来详细讲解PPO算法的可能训练过程。想象一个简单的迷宫，目标是从起点到达终点，迷宫中有固定的障碍物。智能体可以采取的动作有四种：向上走、向下走、向左走、向右走。为了简化，我们假设每走一步得到的即时奖励是-1（鼓励智能体尽快到达终点），到达终点的奖励是+20，撞到墙壁则为-5（作为惩罚）。(PS:本项目实际操作时对奖励的设置进行了调整)
1. **初始探索**：
   - 智能体在初始阶段随机探索，例如它尝试向左走一步。
   - 如果向左走撞到了墙壁，它会得到-5的奖励。这时，PPO算法会收集这一状态、动作和奖励的信息。
2. **计算优势**：
   - 根据收集到的数据，PPO算法会计算每个动作的优势。在这个例子中，向左走撞墙的动作，相比其他可能的动作（比如向右走可能会接近终点），会被认为是一个低优势动作。
3. **策略更新**：
   - 利用收集到的数据和计算出的优势，PPO开始更新策略。对于撞墙这种低优势的动作，PPO会调整策略，降低未来选择这一动作的概率。
   - 在策略更新过程中，PPO会保证更新的步伐不会过大，即使发现撞墙的动作非常不利，也不会立即将向左走的概率降到极低，而是通过策略比率的截断来温和调整。
4. **进一步探索与学习**：
   - 随着更多的探索和学习，智能体开始理解向着终点方向移动会获得更高的累计奖励。例如，它发现向右走能够避开障碍物，并且逐渐接近终点。
   - 在这个过程中，PPO算法不断优化策略，提高向着终点方向移动的动作概率。
5. **战略优化**：
   - 经过一系列的策略迭代更新后，智能体学会了一个高效的策略来完成迷宫任务。它能够根据当前位置，动态选择最优的动作来最快地达到终点。
   - 在这个过程中，智能体可能会发现一些“捷径”——这些特定的动作序列能够更快地到达终点，这些都是通过PPO算法不断优化和探索得到的。



让我们深入探讨PPO算法的核心特点——**策略比率的截断**，并结合之前的走迷宫例子来具体解释这一过程包括何时进行截断以及截断的效果。
### 策略比率的截断
在PPO算法中，策略比率（即新策略与旧策略之间的动作概率比率）的截断是一种确保策略更新步伐既不过大也不过小的关键机制。这种机制的目的是在探索（尝试新动作）和利用（重复已知的好动作）之间找到一个平衡，从而提高学习的稳定性和效率。



假设智能体在探索过程中发现向右走（相对于起始策略）能够有效地接近迷宫的出口，那么根据奖励反馈和优势函数的计算，向右走的动作价值会相对较高。

1. **策略比率计算**：在这个过程中，PPO会计算新策略（更倾向于向右走）与旧策略之间的策略比率。如果智能体在新策略下选择向右的概率是旧策略下的1.5倍，那么策略比率就是1.5。

2. **截断机制**：
   - **截断前**：如果策略比率在一个接受的范围内（比如\[1-ε, 1+ε\]，其中ε通常设为0.2），则智能体会接受这一更新，增加向右走的倾向。
   - **截断点**：若策略比率超出了这个范围，说明策略更新过于激进，可能导致学习不稳定。PPO会对这种过度更新进行“截断”，将实际使用的比率限制在设定的阈值内。这意味着，即使新策略非常倾向于某个动作，其影响也会被人为减弱，以避免过快地偏离旧策略。
   - **截断后**：这种截断保证了策略更新既能吸收新的学习成果，又不会因为一次更新过度而可能导致的负面影响。
   
3. **实例效果**：
   - 如果在某次迭代中，智能体尝试向右走，并且这一动作显著地提高了到达终点的效率，PPO会增加未来选择此动作的概率。但是，如果这一改变太大，超出了预定的安全范围（比如策略比率大于1.2），PPO通过截断机制，限制这一概率的增加，从而避免过激的策略变动，确保学习过程的平稳进行。
   
4. **特定场景下的不截断**：
   - 如果策略比率的变化在安全范围内，说明策略更新既不过激也符合学习目标，这时PPO不会对其进行截断，允许策略按照这一比率更新。这样既保证了策略更新的有效性，又避免了可能的不稳定性。

     

通过这种方式，PPO算法精确控制了策略更新的幅度，确保了高效和稳定的学习过程。策略比率的截断机制是PPO与其他强化学习算法相比的显著特点，它既能促进有效的策略探索，又能防止由于过度更新导致的潜在学习不稳定问题。
